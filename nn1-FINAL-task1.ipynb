{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b2tWBjLyYDgH"
   },
   "source": [
    "# Project 3: Text Classification in PyTorch\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* All the tasks that you need to complete in this project are either coding tasks (mentioned inside the code cells of the notebook with `#TODO` notations) or theoretical questions that you need to answer by editing the markdown question cells.\n",
    "* **Please make sure you read the [Notes](#Important-Notes) section carefully before you start the project.**\n",
    "\n",
    "## Introduction\n",
    "This project deals with neural text classification using PyTorch. Text classification is the process of assigning tags or categories to text according to its content. It's one of the fundamental tasks in Natural Language Processing (NLP) with broad applications such as sentiment analysis, topic labeling, spam detection, and intent detection.\n",
    "\n",
    "Text classification algorithms are at the heart of a variety of software systems that process text data at scale. Email software uses text classification to determine whether incoming mail is sent to the inbox or filtered into the spam folder. Discussion forums use text classification to determine whether comments should be flagged as inappropriate.\n",
    "\n",
    "**_Example:_** A simple example of text classification would be Spam Classification. Consider the bunch of emails that you would receive in the your personal inbox if the email service provider did not have a spam filter algorithm. Because of the spam filter, spam emails get redirected to the Spam folder, while you receive only non-spam (\"_ham_\") emails in your inbox.\n",
    "\n",
    "![](http://blog.yhat.com/static/img/spam-filter.png)\n",
    "\n",
    "## Task\n",
    "Here, we want you to focus on a specific type of text classification task, \"Document Classification into Topics\". It can be addressed as classifying text data or even large documents into separate discrete topics/genres of interest.\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*YWEqFeKKKzDiNWy5UfrTsg.png)\n",
    "\n",
    "In this project, you will be working on classifying given text data into discrete topics or genres. You are given a bunch of text data, each of which has a label attached. We ask you to learn why you think the contents of the documents have been given these labels based on their words. You need to create a neural classifier that is trained on this given information. Once you have a trained classifier, it should be able to predict the label for any new document or text data sample that is fed to it. The labels need not have any meaning to us, nor to you necessarily.\n",
    "\n",
    "## Data\n",
    "There are various datasets that we can use for this purpose. This tutorial shows how to use the text classification datasets in the PyTorch library ``torchtext``. There are different datasets in this library like `AG_NEWS`, `SogouNews`, `DBpedia`, and others. This project will deal with training a supervised learning algorithm for classification using one of these datasets. In task 1 of this project, we will work with the `AG_NEWS` dataset.\n",
    "\n",
    "## Load Data\n",
    "\n",
    "A bag of **ngrams** feature is applied to capture some partial information about the local word order. In practice, bi-grams or tri-grams are applied to provide more benefits as word groups than only one word.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "*\"I love Neural Networks\"*\n",
    "* **Bi-grams:** \"I love\", \"love Neural\", \"Neural Networks\"\n",
    "* **Tri-grams:** \"I love Neural\", \"love Neural Networks\"\n",
    "\n",
    "In the code below, we have loaded the `AG_NEWS` dataset from the ``torchtext.datasets.TextClassification`` package with bi-grams feature. The dataset supports the ngrams method. By setting ngrams to 2, the example text in the dataset will be a list of single words plus bi-grams string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "BfN0KYSqYDgQ",
    "outputId": "b2641cca-12d5-4714-cc16-01d65a66df01"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:17, 6936.55lines/s]\n",
      "120000lines [00:30, 3963.45lines/s]\n",
      "7600lines [00:01, 4166.89lines/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the AG_NEWS dataset in bi-gram features format.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import text_classification\n",
    "import os\n",
    "\n",
    "NGRAMS = 2\n",
    "\n",
    "if not os.path.isdir('./.data'):\n",
    "    os.mkdir('./.data')\n",
    "\n",
    "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "    root='./.data', ngrams=NGRAMS, vocab=None)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JQzNepK5YDgg"
   },
   "source": [
    "## Model\n",
    "\n",
    "Our first simple model is composed of an [`EmbeddingBag`](https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag) layer and a linear layer.\n",
    "\n",
    "``EmbeddingBag`` computes the mean value of a “bag” of embeddings. The text entries here have different lengths. ``EmbeddingBag`` requires no padding here since the text lengths are saved in offsets. Additionally, since ``EmbeddingBag`` accumulates the average across the embeddings on the fly, ``EmbeddingBag`` can enhance the performance and memory efficiency to process a sequence of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T37mlBHjYDgj"
   },
   "outputs": [],
   "source": [
    "# TODO: Import the necessary libraries\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# TODO: Create a class TextClassifier. Remember that this class will be your model.\n",
    "class TextClassifier(nn.Module):\n",
    "    # TODO: Define the __init__() method with proper parameters\n",
    "    # (vocabulary size, dimensions of the embeddings, number of classes)\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        # TODO: define the embedding layer\n",
    "        super().__init__()\n",
    "        self.bag = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "\n",
    "        # TODO: define the linear forward layer\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        \n",
    "        # TODO: Initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    # TODO: Define a method to initialize weights.\n",
    "    def init_weights(self):\n",
    "        # The weights should be random in the range of -0.5 to 0.5.\n",
    "        # You can initialize bias values as zero.\n",
    "        self.bag.weight.data.uniform_(-0.5, 0.5)\n",
    "        self.fc.weight.data.uniform_(-0.5, 0.5)\n",
    "        self.fc.bias.data.zero_()\n",
    "    \n",
    "    # TODO: Define the forward function.\n",
    "    def forward(self, text, offsets):\n",
    "        # This should calculate the embeddings and return the linear layer\n",
    "        # with calculated embedding values.\n",
    "        return self.fc(self.bag(text, offsets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1PjotPB4YDgp"
   },
   "source": [
    "## Check your data before you proceed!\n",
    "\n",
    "Okay, so we know that we are using the `AG_NEWS` dataset in this project, but do you know what does the data contain? What is the format of the data? How many classes of data are there in this dataset? We do not know, yet. Let's find out!\n",
    "\n",
    "\n",
    "## Question 1:\n",
    "Create a new cell in this notebook and try to analyze the dataset that we loaded for you before. Report the following:\n",
    "* Vocabulary size (VOCAB_SIZE)\n",
    "* Number of classes (NUM_CLASS)\n",
    "* Names of the classes\n",
    "\n",
    "\n",
    "## Answer 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V0tG0XEiYDgq",
    "outputId": "1264f53a-7d1b-4385-f31a-cea1c6ade0a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size    : 1308844\n",
      "Classes            : 4\n",
      "- 0 : World\n",
      "- 1 : Sports\n",
      "- 2 : Business\n",
      "- 3 : Sci/Tec\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(train_dataset.get_vocab())\n",
    "print(\"Vocabulary Size    :\", vocab_size)\n",
    "classes = train_dataset.get_labels()\n",
    "\n",
    "print(\"Classes            :\", len(classes))\n",
    "label = {1 : \"World\",\n",
    "         2 : \"Sports\",\n",
    "         3 : \"Business\",\n",
    "         4 : \"Sci/Tec\"}\n",
    "\n",
    "for i in classes:\n",
    "    print(\"-\", i, \":\", label[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cffui5feYDgw"
   },
   "source": [
    "## Create an instance for your model\n",
    "\n",
    "Great! You have successfully completed a basic analysis of the data that you are going to work with. The vocab size is equal to the length of vocab (including single word and ngrams). The number of classes is equal to the number of labels. Copy paste the code statements you used in your analysis to complete the code below. Also, using these parameters, create an instance `model` of your text classifier `TextClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pmuo_rZxYDgx"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Paramters and model instance creation.\n",
    "'''\n",
    "\n",
    "# TODO: Instantiate the Vocabulary size and the number of classes\n",
    "# from the training dataset that we loaded for you.\n",
    "\n",
    "# Hint: Remember that these are PyTorch datasets. So, there should be \n",
    "# readily available functions that you can use to save time. ;)\n",
    "\n",
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "EMBED_DIM = 32\n",
    "NUM_CLASS = len(classes)\n",
    "\n",
    "# TODO: Instantiate the model with the parameters you defined above. \n",
    "# Remember to allocate it to your 'device' variable.\n",
    "\n",
    "model = TextClassifier(VOCAB_SIZE, EMBED_DIM, NUM_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZqzXt-IYDg2"
   },
   "source": [
    "## Generate batch\n",
    "\n",
    "Since the text entries have different lengths, you need to create a custom function to generate data batches and offsets. This function should be passed to the ``collate_fn`` parameter in the ``DataLoader`` call of pyTorch which you will use to create the data later on. The input to ``collate_fn`` is a list of tensors with the size of batch_size, and the ``collate_fn`` function packs them into a mini-batch. Pay attention here and make sure that ``collate_fn`` is declared as a top level definition. This ensures that the function is available in each worker. This is the reason why you need to define this custom function first before you call DataLoader().\n",
    "\n",
    "The text entries in the original data batch input are packed into a list and concatenated as a single tensor as the input of ``EmbeddingBag``. The offsets is a tensor of delimiters to represent the beginning index of the individual sequence in the text tensor. Label is a tensor saving the labels of individual text entries.\n",
    "\n",
    "Finish the function definition below. The function should take batch as an input parameter. Each entry in the batch contains a pair of values of the text and the corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BrH_d9_jYDg3"
   },
   "outputs": [],
   "source": [
    "# TODO: Finish the function definition.\n",
    "\n",
    "def generate_batch(batch):\n",
    "    \n",
    "    label = torch.tensor([i[0] for i in batch])\n",
    "    text = [i[1] for i in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    \n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ycq8NoeYDg8"
   },
   "source": [
    "## Define the train function\n",
    "\n",
    "Here, you need to define a function which you will use later on in the project to train your model. This is very similar to the training steps that you have encountered before in previous coding assignment(s). The outline of the function is something like this -\n",
    "\n",
    "* load the data as batches\n",
    "* iterate over the batches\n",
    "* find the model output for a forward pass\n",
    "* calculate the loss\n",
    "* perform backpropagation on the loss (optimize it)\n",
    "* find the training accuracy\n",
    "\n",
    "In addition to this, you also need to find the total loss and total training accuracy values. Also, you need to return the average values of the total loss and total accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V6WT6DuiYDg9"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train(train_data):\n",
    "\n",
    "    # Initial values of training loss and training accuracy\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "\n",
    "    # TODO: Use the PyTorch DataLoader class to load the data \n",
    "    # into shuffled batches of appropriate sizes into the variable 'data'.\n",
    "    # Remember, this is the place where you need to generate batches.\n",
    "    data = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
    "    \n",
    "    \n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        \n",
    "        # TODO: What do you need to do in order to perform backprop on the optimizer?\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        \n",
    "        # TODO: Store the output of the model in variable 'output'\n",
    "        output = model(text, offsets)\n",
    "        \n",
    "        \n",
    "        # TODO: Define the 'loss' variable (with respect to 'output' and 'cls').\n",
    "        # Also calculate the total loss in variable 'train_loss'\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # TODO: Perform the backward propagation on 'loss' and \n",
    "        # optimize it through the 'optimizer' step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # TODO: Calculate and store the total training accuracy\n",
    "        # in the variable 'total_acc'.\n",
    "        # Remember, you need to find the \n",
    "        train_acc += (output.argmax(1) == cls).sum().item()        \n",
    "\n",
    "    # TODO: Adjust the learning rate here using the scheduler step\n",
    "    scheduler.step()\n",
    "    \n",
    "\n",
    "    return train_loss/len(train_data), train_acc/len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7DLB6J9YDhC"
   },
   "source": [
    "## Define the test function\n",
    "\n",
    "Using the framework of the `train()` function in the previous cell, try to figure out the structure of the test function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5gDJT8zHYDhD"
   },
   "outputs": [],
   "source": [
    "def test(test_data):\n",
    "    \n",
    "    # Initial values of test loss and test accuracy\n",
    "    \n",
    "    test_loss = 0\n",
    "    acc = 0\n",
    "    \n",
    "    # TODO: Use DataLoader class to load the data\n",
    "    # into non-shuffled batches of appropriate sizes.\n",
    "    # Remember, you need to generate batches here too.\n",
    "    data = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    \n",
    "    \n",
    "    for text, offsets, cls in data:\n",
    "        \n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        \n",
    "        # Hint: There is a 'hidden hint' here. Let's see if you can find it :)\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            \n",
    "            # TODO: Get the model output\n",
    "            output = model(text, offsets)\n",
    "            \n",
    "            \n",
    "            # TODO: Calculate and add the loss to find total 'loss'\n",
    "            loss = criterion(output, cls)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            # TODO: Calculate the accuracy and store it in the 'acc' variable\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "            \n",
    "\n",
    "    return test_loss / len(test_data), acc / len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QuSaeb1AYDhG"
   },
   "source": [
    "## Split the dataset and run the model\n",
    "\n",
    "The original `AG_NEWS` has no validation dataset. For this reason, you need to split the training dataset into training and validation sets with a proper split ratio. The `random_split()` function in the torch.utils core PyTorch library should be able to help you with this. We have already imported it for you. :)\n",
    "\n",
    "* Consider the initial learning rate as 4.0, number of epochs as 5, training data ratio as 0.9.\n",
    "* You need to define and use a proper loss function\n",
    "* Define an Optimization algorithm (Suggestion: SGD)\n",
    "* Define a scheduler function to adjust the learning rate through epochs (gamma parameter = 0.9).\n",
    "(Hint: Look at the `StepLR` function)\n",
    "* Monitor the loss and accuracy values for both training and validation data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zc4ozJ6RYDhG",
    "outputId": "ecc07210-c481-4554-8396-054653d69ecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 1 minutes, 18 seconds\n",
      "\tLoss: 0.0265(train)\t|\tAcc: 84.4%(train)\n",
      "\tLoss: 0.0185(valid)\t|\tAcc: 90.1%(valid)\n",
      "Epoch: 2  | time in 1 minutes, 16 seconds\n",
      "\tLoss: 0.0119(train)\t|\tAcc: 93.6%(train)\n",
      "\tLoss: 0.0185(valid)\t|\tAcc: 90.2%(valid)\n",
      "Epoch: 3  | time in 1 minutes, 15 seconds\n",
      "\tLoss: 0.0069(train)\t|\tAcc: 96.4%(train)\n",
      "\tLoss: 0.0193(valid)\t|\tAcc: 90.6%(valid)\n",
      "Epoch: 4  | time in 1 minutes, 15 seconds\n",
      "\tLoss: 0.0038(train)\t|\tAcc: 98.2%(train)\n",
      "\tLoss: 0.0193(valid)\t|\tAcc: 91.2%(valid)\n",
      "Epoch: 5  | time in 1 minutes, 12 seconds\n",
      "\tLoss: 0.0022(train)\t|\tAcc: 99.1%(train)\n",
      "\tLoss: 0.0215(valid)\t|\tAcc: 91.1%(valid)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "# TODO: Set the number of epochs and the learning rate to \n",
    "# their initial values here\n",
    "\n",
    "N_EPOCHS = 5\n",
    "LEARNING_RATE = 4\n",
    "TRAIN_RATIO = 0.9\n",
    "\n",
    "# TODO: Set the intial validation loss to positive infinity\n",
    "validation_loss = float('inf')\n",
    "\n",
    "\n",
    "# TODO: Use the appropriate loss function\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "\n",
    "# TODO: Use the appropriate optimization algorithm with parameters (Suggested: SGD)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "# TODO: Use a scheduler function\n",
    "# gamma parameter = 0.9\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "\n",
    "# TODO: Split the data into train and validation sets using random_split()\n",
    "# Assumed to be 80:20\n",
    "train_n = int(len(train_dataset) * TRAIN_RATIO)\n",
    "training_data, val_data = random_split(train_dataset, [train_n, len(train_dataset) - train_n])\n",
    "\n",
    "\n",
    "# TODO: Finish the rest of the code below\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train(training_data)\n",
    "    valid_loss, valid_acc = test(val_data)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MCIlFdGXYDhN"
   },
   "source": [
    "## Let's  check the test loss and test accuracy\n",
    "\n",
    "So you have trained your model and seen how well it performs on the training and validation datasets. Now, you need to check your model's performance against the test dataset. Using the test dataset as input, report the test loss and test accuracy scores of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dEYUW-EKYDhO",
    "outputId": "857acdfd-c27b-4780-ad72-b5405e96d47b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset...\n",
      "\tLoss: 0.0324(test)\t|\tAcc: 87.2%(test)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compete the code below to find \n",
    "# the results (loss and accuracy) on the test data\n",
    "\n",
    "print('Checking the results of test dataset...')\n",
    "test_loss, test_acc = test(test_dataset)\n",
    "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VrYGOvjdYDhR",
    "outputId": "27c89737-3212-441e-b864-f8af58f0a12f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 'Sports' news\n"
     ]
    }
   ],
   "source": [
    "# importing necessary libraries\n",
    "\n",
    "import re\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# labels for the AG_NEWS dataset\n",
    "\n",
    "ag_news_label = {1 : \"World\",\n",
    "                 2 : \"Sports\",\n",
    "                 3 : \"Business\",\n",
    "                 4 : \"Sci/Tec\"}\n",
    "\n",
    "def predict(text, model, vocab, ngrams):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor([vocab[token]\n",
    "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "vocab = train_dataset.get_vocab()\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "# TODO: Predict the topic of the above given random text (use bigrams)\n",
    "# Use the proper paramters in the predict() function\n",
    "\n",
    "print(\"This is a '%s' news\" % ag_news_label[predict(ex_text_str, model, vocab, 2)])\n",
    "\n",
    "# If you have done everything correctly in this task,\n",
    "# then the output of this cell should be - \"This is a 'Sports' news\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NLTc3YL-YDhT"
   },
   "source": [
    "# Congratulations! You just designed your first neural classifier!\n",
    "\n",
    "And probably you have achieved a good accuracy score too. Great job!\n",
    "\n",
    "## Question 2:\n",
    "You just tested your model with a new sample text. Try to feed some more random examples of similar text (which you think are related to at least one of the four topics _\"World\", \"Sports\", \"Business\", \"Sci/Tec\"_ of our problem) to the model and see how your model reacts. Give at least 3 such examples (You are free to include more examples if you wish to).\n",
    "\n",
    "## Answer 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mbVXu1XOYDhU",
    "outputId": "3270fa36-75a6-4772-c87f-f854f7e478f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 'World' news\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://www.theguardian.com/uk-news/2020/jan/13/queen-gives-reluctant-blessing-to-harry-and-meghans-plans\n",
    "pred_world = \"Queen gives reluctant blessing to Harry and Meghan's plans. \\\n",
    "She agreed to a ‘period of transition’ and stressed the couple remain ‘a valued part of my family’. \\\n",
    "The Queen has given her reluctant blessing to the Duke and Duchess of Sussex to split their time between the \\\n",
    "UK and Canada, making it clear that though she had wanted the couple to remain as full-time working royals, she supported their decision.\\\n",
    "After a historic summit of senior royals at Sandringham, details over exactly how Harry and Meghan will \\\n",
    "carve out the new “progressive” roles they seek remained unclear. The Queen has, however, agreed to a \\\n",
    "“period of transition” and stressed the couple remain “a valued part of my family”.\\\n",
    "But there were “complex matters” still to resolve, and “more work to be done” as she\\\n",
    "said she wants final decisions to be reached in the coming days.\\\n",
    "The Queen’s statement came after 90 minutes of talks, which began\\\n",
    "against the backdrop of Prince William and Prince Harry attempting to stem \\\n",
    "rancorous speculation about their relationship in a joint statement.\"\n",
    "\n",
    "print(\"This is a '%s' news\" % ag_news_label[predict(pred_world, model, vocab, 2)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 'Sports' news\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://www.theguardian.com/football/2020/jan/13/barcelona-quique-setien-to-replace-ernesto-valverde\n",
    "pred_sport = \"Barcelona appoint Quique Setién as head coach to replace Ernesto Valverde\\\n",
    "Barcelona have sacked manager Ernesto Valverde and replaced him with the former Real Betis coach \\\n",
    "Quique Setién. Valverde, who had been in charge since the summer of 2017, becomes the first manager \\\n",
    "at the club to be sacked mid-season since Louis van Gaal 17 years ago. He leaves with Barcelona through \\\n",
    "to the knockout phase of the Champions League as group winners and top of La Liga, which they have won for\\\n",
    "each of the last two years. He had six months left on his contract, plus the option for another year after \\\n",
    "that. Setién has been given a two-and-a-half year contract until June 2022.\\\n",
    "How Barcelona made a right mess of sacking Ernesto Valverde. \\\n",
    "After days of openly pursuing replacements, Valverde was finally informed of the club’s intentions \\\n",
    "on Monday evening. During that time Xavi Hernández and Ronald Koeman, both of whom had expressed their \\\n",
    "desire to coach the club in the future, turned down the opportunity to take over with immediate effect. \\\n",
    "Barcelona had contemplated a series of other names, including Thierry Henry and the former Tottenham manager \\\n",
    "Mauricio Pochettino. Almost a dozen managers had been connected to the club as rumours circulated. The impression was of a club not sure which way to turn.\"\n",
    "\n",
    "print(\"This is a '%s' news\" % ag_news_label[predict(pred_sport, model, vocab, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 'Sci/Tec' news\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://www.theguardian.com/technology/2020/jan/13/google-parent-company-alphabet-expected-reach-1-trillion-value\n",
    "pred_tech = \"Google parent company Alphabet expected to reach $1tn value soon.\\\n",
    "Alphabet may join Apple, Microsoft and Amazon when it reports latest earnings, another sign of the unstoppable rise of tech.\\\n",
    "Another tech behemoth is poised to join the club of Silicon Valley giants valued at more than $1tn. Alphabet, Google’s parent \\\n",
    "company, reached a value of $993bn on Monday, with analysts expecting it to cross the $1tn mark soon.\\\n",
    "Alphabet would join a select club of tech companies to pass $1tn in value. Apple became the first tech \\\n",
    "company to pass the benchmark in August 2018 and has since risen to be valued at $1.37tn.\\\n",
    "The iPhone company was followed by Microsoft, which passed $1tn in April 2019 and Amazon, \\\n",
    "which joined the club in September. The value of Microsoft has continued to rise but Amazon has slipped back and is now worth $940bn.\\\n",
    "The five most valuable companies in the US are now all tech companies, with Facebook rounding out the pack with a current market capitalization of $631bn.\"\n",
    "\n",
    "print(\"This is a '%s' news\" % ag_news_label[predict(pred_tech, model, vocab, 2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "whu1tk6pYDhY"
   },
   "source": [
    "## Question 3:\n",
    "Okay, probably the model still works great with the examples you fed to it in the previous question. How about a twist in the plot? Let's feed it some more random text data from completely different genres/topics (not belonging to the 4 topics which we talk about the in the first question). How does your model react now? Give at least 3 such examples (You are free to include more examples if you wish to).\n",
    "\n",
    "Of course the predictions will be limited to the four class labels that your model is trained on. Can you somehow justify the labels that your model predicted now for the given text inputs?\n",
    "\n",
    "## Answer 3a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uyuntzMJYDhZ",
    "outputId": "20911488-32cd-4d96-a1b8-91f272bbe44b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 'World' news\n"
     ]
    }
   ],
   "source": [
    "# Topic: Music, Ref: https://www.theguardian.com/music/2020/jan/11/brit-award-nominations-2020-dave-lewis-capaldi\n",
    "pred_music = \"Brit award nominations 2020: Dave and Lewis Capaldi top pile, with women shut out.\\\n",
    "Only one British woman, Mabel, is nominated across the best album, song and new artist \\\n",
    "categories, which skew heavily in favour of solo men.\\\n",
    "The British music industry’s focus on the male solo artist at the expense of female \\\n",
    "musicians has been thrown into sharp relief by the nominations for the 2020 Brit awards, \\\n",
    "in which only one British woman – and no groups featuring women – was nominated across 25 slots in mixed-gender categories.\\\n",
    "Pop singer Mabel was nominated for best new artist and best song for Don’t Call Me Up – but the nine other song nominees and \\\n",
    "four other new artist nominees are solo British males. (US star Miley Cyrus guests on Mark Ronson’s nominated song Nothing Breaks Like a Heart.)\"\n",
    "\n",
    "print(\"This is a '%s' news\" % ag_news_label[predict(pred_music, model, vocab, 2)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 'Sci/Tec' news\n"
     ]
    }
   ],
   "source": [
    "# Topic: Games, Ref: https://www.theguardian.com/games/2020/jan/09/transport-fever-2-review\n",
    "pred_game = \"Transport Fever 2 review – simple pleasures offer copious fuel for fun. \\\n",
    "There’s much joy to be had building freight networks and watching cities grow … but what about the real-world pitfalls?\\\n",
    "As Britain returns to a daily commute beset with fare hikes and failing rail companies, there is significant appeal to a \\\n",
    "game in which you make the trains run on time. In the same way The Sims allows thirtysomething millennials to experience \\\n",
    "the fantasy of home ownership, so Transport Fever 2 lets you enjoy the thrill of plonking a bullet-train between Brighton and London Victoria.\\\n",
    "The concept of the transport sim is nothing new. Video games have been offering virtual train sets since Sid Meier’s Railroad Tycoon, \\\n",
    "letting players enjoy locomotive logistics without requiring a shed to store all those model networks. But Transport Fever 2 goes \\\n",
    "way beyond laying railroads. Everything from planes to pontoons can be deployed to carry commuters and cargo to your chosen destinations.\"\n",
    "\n",
    "print(\"This is a '%s' news\" % ag_news_label[predict(pred_game, model, vocab, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 'World' news\n"
     ]
    }
   ],
   "source": [
    "# Topic: Film, Ref: https://www.theguardian.com/film/2020/jan/13/oscars-2020-nominations-joker-irishman\n",
    "pred_film = \"Joker leads Oscars 2020 pack – but Academy just trumps Baftas for diversity.\\\n",
    "Less than a week since Bafta’s strikingly white and male awards shortlist met \\\n",
    "with widespread criticism – including from the organisation’s own chief executive \\\n",
    "– the Academy of Motion Picture Arts and Sciences has released a set of nominations \\\n",
    "whose small concessions to diversity seem striking by contrast.\\\n",
    "Cynthia Erivo is nominated for best actress for her role in a biopic of abolitionist Harriet Tubman, \\\n",
    "and Parasite – Bong Joon-ho’s acclaimed South Korean black comedy – is up for six awards, including best director and best picture.\\\n",
    "Little Women, Greta Gerwig’s so-far-overlooked take on the Louisa May Alcott classic, also scored six nominations, \\\n",
    "including best picture and best adapted screenplay – but Gerwig was locked out of the all-male best director shortlist.\"\n",
    "\n",
    "\n",
    "print(\"This is a '%s' news\" % ag_news_label[predict(pred_film, model, vocab, 2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 3.b:\n",
    "Of course the predictions will be limited to the four class labels that your model is trained on. Can you somehow justify the labels that your model predicted now for the given text inputs?\n",
    "\n",
    "\n",
    "Answers are in the report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BbR5n3m4YDhc"
   },
   "source": [
    "## Question 4:\n",
    "Your model probably has achieved a good accuracy score. However, there may be lots of things that you could still try to do to improve your classifier model. Can you try to list down some improvements that you think would be able to improve the above model's performance?\n",
    "\n",
    "_(Hint: Maybe think about alternate architectures, #layers, hyper-paramters, etc..., but try not to come up with too complex stuff! :) )_\n",
    "\n",
    "## Answer 4:\n",
    "Answers are in the report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nW_w_xf5YDhj"
   },
   "source": [
    "\n",
    "\n",
    "# Important Notes\n",
    "\n",
    "## NOTE 1:\n",
    "If you want, you can try out the models on other datasets too for comparisons. Although this is not mandatory, it would be really interesting to see how your model performs for data from different domains maybe. Note that you may need to tweak the code a little bit when you are considering other datasets and formats. \n",
    "\n",
    "## NOTE 2:\n",
    "Any form of plagiarism is strictly prohibited. If it is found that you have copied sample code from the internet, the entire team will be penalized.\n",
    "\n",
    "## NOTE 3:\n",
    "Often Jupyter Notebooks tend to stop working or crash due to overload of memory (lot of variables, big neural models, memory-intensive training of models, etc...). Moreover, with more number of tasks, the number of variables that you will be using will surely incerase. Therefore, it is recommended that you use separate notebooks for each _Task_ in this project.\n",
    "\n",
    "## NOTE 4:\n",
    "You are expected to write well-documented code, that is, with proper comments wherever you think is needed. Make sure you write a comprehensive report for the entire project consisting of data analysis, your model architecture, methods used, discussing and comparing the models against the accuracy and loss metrics, and a final conslusion. If you want to prepare separate reports for each _Task_, you could do this in the Jupyter Notebook itself using $Mardown$ and $\\LaTeX$ code if needed. If you want to submit a single report for the entire project, you could submit a PDF file in that case (Word or $\\LaTeX$).\n",
    "\n",
    "All the very best for project 2. Wishing you happy holidays and a very happy new year in advance! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TUUvW1nOYDhk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Project_2_(NLP_Text_Classification).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
