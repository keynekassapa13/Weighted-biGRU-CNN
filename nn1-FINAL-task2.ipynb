{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b2tWBjLyYDgH"
   },
   "source": [
    "# Project 3: Text Classification in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZVD7KlsSYDhd"
   },
   "source": [
    "# Task 2: Try the better option that you proposed\n",
    "\n",
    "In Question 4, you have proposed some alternate solution that you think will be able to somehow improve your model. Following one of the options below, try to build and train a new model, and report the new loss and accuracy scores. Is it better than your initial classifier model for the same data?\n",
    "\n",
    "For your reference, here are some neural models using which researchers have tried to classify text before:\n",
    "\n",
    "* Recurrent Neural Networks (RNNs)\n",
    "* Long-Short Term Memory (LSTM)\n",
    "* Bi-directional LSTM (BiLSTM)\n",
    "* Gated Recurrent Units (GRUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:16, 7257.64lines/s]\n",
      "120000lines [00:29, 4040.97lines/s]\n",
      "7600lines [00:01, 4139.34lines/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import text_classification\n",
    "import os\n",
    "\n",
    "NGRAMS = 2\n",
    "\n",
    "if not os.path.isdir('./.data'):\n",
    "    os.mkdir('./.data')\n",
    "\n",
    "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "    root='./.data', ngrams=NGRAMS, vocab=None)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "VOCAB_SIZE = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vocab_size = len(train_dataset.get_vocab())\n",
    "classes = train_dataset.get_labels()\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "EMBED_DIM = 32\n",
    "NUM_CLASS = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    \n",
    "    label = torch.tensor([i[0] for i in batch])\n",
    "    text = [i[1] for i in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    \n",
    "    return text, offsets, label\n",
    "\n",
    "def train(train_data):\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "\n",
    "    data = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
    "    \n",
    "    \n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = model(text, offsets)\n",
    "        \n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "        \n",
    "    scheduler.step()\n",
    "    \n",
    "\n",
    "    return train_loss/len(train_data), train_acc/len(train_data)\n",
    "\n",
    "def test(test_data):\n",
    "    \n",
    "    test_loss = 0\n",
    "    acc = 0\n",
    "    \n",
    "    data = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    \n",
    "    for text, offsets, cls in data:\n",
    "        \n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "            test_loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return test_loss / len(test_data), acc / len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Wang \n",
    "\n",
    "#### Brief description and analysis is in the report\n",
    "\n",
    "TLDR;\n",
    "\n",
    "    out perform on test accuracy\n",
    "    not out perform on training and validation accuracy\n",
    "\n",
    "Epoch: 1\n",
    "\n",
    "\tLoss: 0.0697(train)\t|\tAcc: 60.3%(train)\n",
    "\tLoss: 0.0558(valid)\t|\tAcc: 84.9%(valid)\n",
    "\n",
    "Epoch: 2\n",
    "\n",
    "\tLoss: 0.0551(train)\t|\tAcc: 86.1%(train)\n",
    "\tLoss: 0.0541(valid)\t|\tAcc: 87.7%(valid)\n",
    "\n",
    "Epoch: 3\n",
    "\n",
    "\tLoss: 0.0533(train)\t|\tAcc: 89.0%(train)\n",
    "\tLoss: 0.0537(valid)\t|\tAcc: 88.2%(valid)\n",
    "\n",
    "Epoch: 4\n",
    "\n",
    "\tLoss: 0.0527(train)\t|\tAcc: 90.0%(train)\n",
    "\tLoss: 0.0535(valid)\t|\tAcc: 88.6%(valid)\n",
    "\n",
    "Epoch: 5\n",
    "\n",
    "\tLoss: 0.0521(train)\t|\tAcc: 90.8%(train)\n",
    "\tLoss: 0.0532(valid)\t|\tAcc: 89.2%(valid)\n",
    "\n",
    "Performance on test dataset\n",
    "\n",
    "    Loss: 0.0532(test)\t|\tAcc: 89.0%(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.bag = nn.Embedding(vocab_size, embed_dim, sparse=True)\n",
    "        self.lstm = nn.LSTM(embed_dim, embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.sf = nn.Softmax()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.weight = torch.nn.Parameter(torch.Tensor(32, 1).uniform_(-0.0001, 0.0001) * torch.sqrt(torch.tensor(6./32+1)))\n",
    "        self.bag.weight.data.uniform_(-0.5, 0.5)\n",
    "        self.fc.weight.data.uniform_(-0.5, 0.5)\n",
    "        self.fc.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, text, offsets):\n",
    "        \n",
    "        text_offsets = []\n",
    "        for i in range(1, len(offsets)+1):\n",
    "            if i == len(offsets):\n",
    "                text_offsets.append(text[offsets[i-1]:])\n",
    "            else:\n",
    "                text_offsets.append(text[offsets[i-1]:offsets[i]])\n",
    "        text_len = [len(x) for x in text_offsets]\n",
    "          \n",
    "        sorted_text_len = sorted(text_len)\n",
    "        max_length = sorted_text_len[13]\n",
    "        \n",
    "        text_len = []\n",
    "        for x in text_offsets:\n",
    "            tmp = len(x) if len(x) < max_length else max_length\n",
    "            text_len.append(tmp)\n",
    "\n",
    "        pad_text = nn.utils.rnn.pad_sequence(text_offsets)\n",
    "        pad_text = pad_text[:max_length, :]\n",
    "        x = self.bag(pad_text)\n",
    "    \n",
    "        x_packed = pack_padded_sequence(x, text_len, enforce_sorted=False)\n",
    "        lstm_packed, (hn, cn) = self.lstm(x_packed)\n",
    "        lstm_output, input_sizes = pad_packed_sequence(lstm_packed)\n",
    "\n",
    "        lstm_output = lstm_output.resize(max_length, 16, 32, 1)\n",
    "        lstm_output = lstm_output * self.weight\n",
    "        lstm_output = lstm_output.resize(max_length, 16, 32)\n",
    "        \n",
    "        sum_output = torch.sum(lstm_output, dim=0)\n",
    "        out = self.fc(sum_output)\n",
    "        out = self.sf(out)\n",
    "        return out\n",
    "\n",
    "model = TextClassifier(VOCAB_SIZE, EMBED_DIM, NUM_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keynekassapa13/.local/lib/python3.7/site-packages/torch/tensor.py:339: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 21 minutes, 8 seconds\n",
      "\tLoss: 0.0697(train)\t|\tAcc: 60.3%(train)\n",
      "\tLoss: 0.0558(valid)\t|\tAcc: 84.9%(valid)\n",
      "Epoch: 2  | time in 23 minutes, 9 seconds\n",
      "\tLoss: 0.0551(train)\t|\tAcc: 86.1%(train)\n",
      "\tLoss: 0.0541(valid)\t|\tAcc: 87.7%(valid)\n",
      "Epoch: 3  | time in 24 minutes, 58 seconds\n",
      "\tLoss: 0.0533(train)\t|\tAcc: 89.0%(train)\n",
      "\tLoss: 0.0537(valid)\t|\tAcc: 88.2%(valid)\n",
      "Epoch: 4  | time in 26 minutes, 7 seconds\n",
      "\tLoss: 0.0527(train)\t|\tAcc: 90.0%(train)\n",
      "\tLoss: 0.0535(valid)\t|\tAcc: 88.6%(valid)\n",
      "Epoch: 5  | time in 30 minutes, 12 seconds\n",
      "\tLoss: 0.0521(train)\t|\tAcc: 90.8%(train)\n",
      "\tLoss: 0.0532(valid)\t|\tAcc: 89.2%(valid)\n",
      "Checking the results of test dataset...\n",
      "\tLoss: 0.0532(test)\t|\tAcc: 89.0%(test)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "N_EPOCHS = 5\n",
    "LEARNING_RATE = 0.6\n",
    "TRAIN_RATIO = 0.9\n",
    "\n",
    "validation_loss = float('inf')\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "train_n = int(len(train_dataset) * TRAIN_RATIO)\n",
    "training_data, valid_data = random_split(train_dataset, [train_n, len(train_dataset) - train_n])\n",
    "\n",
    "for epoch in range(5):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train(training_data)\n",
    "    valid_loss, valid_acc = test(valid_data)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')\n",
    "    \n",
    "print('Checking the results of test dataset...')\n",
    "test_loss, test_acc = test(test_dataset)\n",
    "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Li\n",
    "\n",
    "#### Brief description and analysis is in the report\n",
    "\n",
    "TLDR;\n",
    "\n",
    "    out perform on test accuracy\n",
    "    relatively same on validation accuracy\n",
    "    not out perform on training accuracy\n",
    "\n",
    "Epoch: 1\n",
    "\n",
    "\tLoss: 0.0737(train)\t|\tAcc: 41.6%(train)\n",
    "\tLoss: 0.0500(valid)\t|\tAcc: 62.4%(valid)\n",
    "    \n",
    "Epoch: 2\n",
    "\n",
    "\tLoss: 0.0329(train)\t|\tAcc: 80.2%(train)\n",
    "\tLoss: 0.0242(valid)\t|\tAcc: 87.7%(valid)\n",
    "\n",
    "Epoch: 3\n",
    "\n",
    "\tLoss: 0.0208(train)\t|\tAcc: 88.9%(train)\n",
    "\tLoss: 0.0191(valid)\t|\tAcc: 89.9%(valid)\n",
    "    \n",
    "Epoch: 4\n",
    "\n",
    "\tLoss: 0.0168(train)\t|\tAcc: 91.1%(train)\n",
    "\tLoss: 0.0183(valid)\t|\tAcc: 90.2%(valid)\n",
    "    \n",
    "Epoch: 5 \n",
    "\n",
    "\tLoss: 0.0139(train)\t|\tAcc: 92.6%(train)\n",
    "\tLoss: 0.0187(valid)\t|\tAcc: 90.9%(valid)\n",
    "    \n",
    "    \n",
    "Performance on test dataset\n",
    "\n",
    "\tLoss: 0.0205(test)\t|\tAcc: 89.0%(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.bag = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, embed_dim//2, bidirectional=True)\n",
    "        \n",
    "        self.conv = nn.Conv2d(1, 16, (3, embed_dim//2), stride=1, bias=True)\n",
    "        self.conv1 = nn.Conv2d(1, 16, (4, embed_dim//2), stride=1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(1, 16, (5, embed_dim//2), stride=1, bias=True)\n",
    "        \n",
    "        self.fc = nn.Linear(48, 4)\n",
    "    \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.bag.weight.data.uniform_(-0.5, 0.5)\n",
    "        self.fc.weight.data.uniform_(-0.5, 0.5)\n",
    "        self.fc.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, text, offsets):\n",
    "        \n",
    "        text_offsets = []\n",
    "        for i in range(1, len(offsets)+1):\n",
    "            if i == len(offsets):\n",
    "                text_offsets.append(text[offsets[i-1]:])\n",
    "            else:\n",
    "                text_offsets.append(text[offsets[i-1]:offsets[i]])\n",
    "        text_len = [len(x) for x in text_offsets]\n",
    "\n",
    "        pad_text = nn.utils.rnn.pad_sequence(text_offsets)\n",
    "        \n",
    "        x = self.bag(pad_text)\n",
    "        x_packed = pack_padded_sequence(x, text_len, enforce_sorted=False)\n",
    "        \n",
    "        # GRU\n",
    "        gru_packed, hn = self.gru(x_packed)\n",
    "        \n",
    "        output, input_sizes = pad_packed_sequence(gru_packed)\n",
    "        \n",
    "        oz = output.size()\n",
    "        output = output.resize(oz[0], oz[1], oz[2], 1)\n",
    "        output = output.transpose(1, 3)\n",
    "        output = output.transpose(2, 3)\n",
    "        output = output.transpose(0, 2)\n",
    "        \n",
    "        output_1 = output[:, :, :, :16]\n",
    "        output_2 = output[:, :, :, 16:]\n",
    "        output = output_1.add(output_2)\n",
    "        \n",
    "        # CONV\n",
    "        out_conv1 = self.conv(output)\n",
    "        out_conv1, _ = torch.max(out_conv1, dim=2)\n",
    "        \n",
    "        out_conv2 = self.conv(output)\n",
    "        out_conv2, _ = torch.max(out_conv2, dim=2)\n",
    "        \n",
    "        out_conv3 = self.conv(output)\n",
    "        out_conv3, _ = torch.max(out_conv3, dim=2)\n",
    "    \n",
    "        out_conv = torch.cat([out_conv1, out_conv2, out_conv3], dim=1)\n",
    "        out = self.fc(out_conv.squeeze(2))\n",
    "        return out\n",
    "\n",
    "model = TextClassifier(VOCAB_SIZE, EMBED_DIM, NUM_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 101 minutes, 45 seconds\n",
      "\tLoss: 0.0737(train)\t|\tAcc: 41.6%(train)\n",
      "\tLoss: 0.0500(valid)\t|\tAcc: 62.4%(valid)\n",
      "Epoch: 2  | time in 102 minutes, 31 seconds\n",
      "\tLoss: 0.0329(train)\t|\tAcc: 80.2%(train)\n",
      "\tLoss: 0.0242(valid)\t|\tAcc: 87.7%(valid)\n",
      "Epoch: 3  | time in 102 minutes, 43 seconds\n",
      "\tLoss: 0.0208(train)\t|\tAcc: 88.9%(train)\n",
      "\tLoss: 0.0191(valid)\t|\tAcc: 89.9%(valid)\n",
      "Epoch: 4  | time in 103 minutes, 7 seconds\n",
      "\tLoss: 0.0168(train)\t|\tAcc: 91.1%(train)\n",
      "\tLoss: 0.0183(valid)\t|\tAcc: 90.2%(valid)\n",
      "Epoch: 5  | time in 93 minutes, 32 seconds\n",
      "\tLoss: 0.0139(train)\t|\tAcc: 92.6%(train)\n",
      "\tLoss: 0.0187(valid)\t|\tAcc: 90.9%(valid)\n",
      "Checking the results of test dataset...\n",
      "\tLoss: 0.0205(test)\t|\tAcc: 89.0%(test)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "N_EPOCHS = 5\n",
    "LEARNING_RATE = 0.6\n",
    "TRAIN_RATIO = 0.9\n",
    "\n",
    "validation_loss = float('inf')\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "train_n = int(len(train_dataset) * TRAIN_RATIO)\n",
    "training_data, valid_data = random_split(train_dataset, [train_n, len(train_dataset) - train_n])\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train(training_data)\n",
    "    valid_loss, valid_acc = test(valid_data)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Modified Li\n",
    "\n",
    "#### Brief description and analysis is in the report\n",
    "\n",
    "TLDR;\n",
    "   \n",
    "    out perform on test accuracy & validation accuracy\n",
    "    not out perform on training accuracy\n",
    "\n",
    "Epoch: 1  | time in 68 minutes, 57 seconds\n",
    "\n",
    "\tLoss: 0.0366(train)\t|\tAcc: 76.0%(train)\n",
    "\tLoss: 0.0201(valid)\t|\tAcc: 86.6%(valid)\n",
    "\n",
    "Epoch: 2  | time in 42 minutes, 21 seconds\n",
    "\n",
    "\tLoss: 0.0170(train)\t|\tAcc: 90.4%(train)\n",
    "\tLoss: 0.0173(valid)\t|\tAcc: 90.2%(valid)\n",
    "    \n",
    "Epoch: 3  | time in 40 minutes, 8 seconds\n",
    "\n",
    "\tLoss: 0.0131(train)\t|\tAcc: 92.7%(train)\n",
    "\tLoss: 0.0163(valid)\t|\tAcc: 91.1%(valid)\n",
    "\n",
    "Epoch: 4  | time in 31 minutes, 47 seconds\n",
    "\n",
    "\tLoss: 0.0103(train)\t|\tAcc: 94.2%(train)\n",
    "\tLoss: 0.0168(valid)\t|\tAcc: 91.0%(valid)\n",
    "    \n",
    "Epoch: 5  | time in 35 minutes, 12 seconds\n",
    "\n",
    "\tLoss: 0.0077(train)\t|\tAcc: 95.7%(train)\n",
    "\tLoss: 0.0183(valid)\t|\tAcc: 90.8%(valid)\n",
    "    \n",
    "Performance on test dataset\n",
    "\n",
    "\tLoss: 0.0200(test)\t|\tAcc: 90.1%(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.bag = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, embed_dim//2, bidirectional=True)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(2, 8, 3, stride=2, bias=True)\n",
    "        self.conv2 = nn.Conv1d(2, 8, 4, stride=2, bias=True)\n",
    "        self.fc = nn.Linear(7, 4)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.bag.weight.data.uniform_(-0.5, 0.5)\n",
    "        self.fc.weight.data.uniform_(-0.5, 0.5)\n",
    "        self.fc.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, text, offsets):\n",
    "        \n",
    "        text_offsets = []\n",
    "        for i in range(1, len(offsets)+1):\n",
    "            if i == len(offsets):\n",
    "                text_offsets.append(text[offsets[i-1]:])\n",
    "            else:\n",
    "                text_offsets.append(text[offsets[i-1]:offsets[i]])\n",
    "        text_len = [len(x) for x in text_offsets]\n",
    "\n",
    "        pad_text = nn.utils.rnn.pad_sequence(text_offsets)\n",
    "        \n",
    "        x = self.bag(pad_text)\n",
    "        x_packed = pack_padded_sequence(x, text_len, enforce_sorted=False)\n",
    "        \n",
    "        # GRU\n",
    "        gru_packed, hn = self.gru(x_packed)\n",
    "        hn = hn.transpose(0, 1)\n",
    "        \n",
    "        # CONV\n",
    "        out_conv = self.conv1(hn)\n",
    "        out_conv = torch.mean(out_conv, dim=1)\n",
    "        out = self.fc(out_conv)\n",
    "        return out\n",
    "\n",
    "'''\n",
    "Paramters and model instance creation.\n",
    "'''\n",
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "EMBED_DIM = 32\n",
    "NUM_CLASS = len(classes)\n",
    "\n",
    "model = TextClassifier(VOCAB_SIZE, EMBED_DIM, NUM_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 68 minutes, 57 seconds\n",
      "\tLoss: 0.0366(train)\t|\tAcc: 76.0%(train)\n",
      "\tLoss: 0.0201(valid)\t|\tAcc: 88.6%(valid)\n",
      "Epoch: 2  | time in 42 minutes, 21 seconds\n",
      "\tLoss: 0.0170(train)\t|\tAcc: 90.4%(train)\n",
      "\tLoss: 0.0173(valid)\t|\tAcc: 90.2%(valid)\n",
      "Epoch: 3  | time in 40 minutes, 8 seconds\n",
      "\tLoss: 0.0131(train)\t|\tAcc: 92.7%(train)\n",
      "\tLoss: 0.0163(valid)\t|\tAcc: 91.1%(valid)\n",
      "Epoch: 4  | time in 31 minutes, 47 seconds\n",
      "\tLoss: 0.0103(train)\t|\tAcc: 94.2%(train)\n",
      "\tLoss: 0.0168(valid)\t|\tAcc: 91.0%(valid)\n",
      "Epoch: 5  | time in 35 minutes, 12 seconds\n",
      "\tLoss: 0.0077(train)\t|\tAcc: 95.7%(train)\n",
      "\tLoss: 0.0183(valid)\t|\tAcc: 90.8%(valid)\n",
      "Checking the results of test dataset...\n",
      "\tLoss: 0.0200(test)\t|\tAcc: 90.1%(test)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "N_EPOCHS = 5\n",
    "LEARNING_RATE = 0.615\n",
    "TRAIN_RATIO = 0.9\n",
    "\n",
    "validation_loss = float('inf')\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, weight_decay=0.00001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.95)\n",
    "\n",
    "train_n = int(len(train_dataset) * TRAIN_RATIO)\n",
    "training_data, valid_data = random_split(train_dataset, [train_n, len(train_dataset) - train_n])\n",
    "\n",
    "for epoch in range(5):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train(training_data)\n",
    "    valid_loss, valid_acc = test(valid_data)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')\n",
    "    \n",
    "print('Checking the results of test dataset...')\n",
    "test_loss, test_acc = test(test_dataset)\n",
    "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Li with GloVe\n",
    "\n",
    "#### Brief description and analysis is in the report\n",
    "\n",
    "TLDR;\n",
    "\n",
    "    out perform on test accuracy & validation accuracy\n",
    "    not out perform on training accuracy\n",
    "\n",
    "Epoch: 1  | time in 51 minutes, 59 seconds\n",
    "\n",
    "\tLoss: 0.0176(train)\t|\tAcc: 90.3%(train)\n",
    "\tLoss: 0.0143(valid)\t|\tAcc: 92.1%(valid)\n",
    "    \n",
    "Epoch: 2  | time in 42 minutes, 0 seconds\n",
    "\n",
    "\tLoss: 0.0109(train)\t|\tAcc: 94.0%(train)\n",
    "\tLoss: 0.0129(valid)\t|\tAcc: 92.9%(valid)\n",
    "    \n",
    "Epoch: 3  | time in 40 minutes, 38 seconds\n",
    "\n",
    "\tLoss: 0.0074(train)\t|\tAcc: 96.0%(train)\n",
    "\tLoss: 0.0135(valid)\t|\tAcc: 92.9%(valid)\n",
    "    \n",
    "Epoch: 4  | time in 39 minutes, 4 seconds\n",
    "\n",
    "\tLoss: 0.0048(train)\t|\tAcc: 97.4%(train)\n",
    "\tLoss: 0.0156(valid)\t|\tAcc: 92.4%(valid)\n",
    "    \n",
    "Epoch: 5  | time in 39 minutes, 8 seconds\n",
    "\t\n",
    "    Loss: 0.0029(train)\t|\tAcc: 98.4%(train)\n",
    "\tLoss: 0.0202(valid)\t|\tAcc: 91.7%(valid)\n",
    "\n",
    "Performance on test dataset\n",
    "\n",
    "\tLoss: 0.0214(test)\t|\tAcc: 91.5%(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class, pretrained_vectors):\n",
    "        super().__init__()\n",
    "        self.bag = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.bag.load_state_dict({'weight': pretrained_vectors})\n",
    "        \n",
    "        self.rnn = nn.GRU(embed_dim, embed_dim//2, bidirectional=True)\n",
    "        \n",
    "        self.conv = nn.Conv2d(1, 8, (3, embed_dim//2), stride=1, bias=True)\n",
    "        self.conv1 = nn.Conv2d(1, 8, (4, embed_dim//2), stride=1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(1, 8, (5, embed_dim//2), stride=1, bias=True)\n",
    "        \n",
    "        self.fc = nn.Linear(24, 4)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.fc.weight.data.uniform_(-0.5, 0.5)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        \n",
    "        text_offsets = []\n",
    "        for i in range(1, len(offsets)+1):\n",
    "            if i == len(offsets):\n",
    "                text_offsets.append(text[offsets[i-1]:])\n",
    "            else:\n",
    "                text_offsets.append(text[offsets[i-1]:offsets[i]])\n",
    "        text_len = [len(x) for x in text_offsets]\n",
    "\n",
    "        pad_text = nn.utils.rnn.pad_sequence(text_offsets)\n",
    "        \n",
    "        x = self.bag(pad_text)\n",
    "        x_packed = pack_padded_sequence(x, text_len, enforce_sorted=False)\n",
    "\n",
    "        rnn_packed, hn = self.rnn(x_packed)\n",
    "        \n",
    "        output, input_sizes = pad_packed_sequence(rnn_packed)\n",
    "        \n",
    "        oz = output.size()\n",
    "        output = output.resize(oz[0], oz[1], oz[2], 1)\n",
    "        output = output.transpose(1, 3)\n",
    "        output = output.transpose(2, 3)\n",
    "        output = output.transpose(0, 2)\n",
    "        \n",
    "        output_1 = output[:, :, :, :25]\n",
    "        output_2 = output[:, :, :, 25:]\n",
    "        output = output_1.add(output_2)\n",
    "        \n",
    "        # CONV\n",
    "        out_conv1 = self.conv(output)\n",
    "        out_conv1, _ = torch.max(out_conv1, dim=2)\n",
    "        \n",
    "        out_conv2 = self.conv(output)\n",
    "        out_conv2, _ = torch.max(out_conv2, dim=2)\n",
    "        \n",
    "        out_conv3 = self.conv(output)\n",
    "        out_conv3, _ = torch.max(out_conv3, dim=2)\n",
    "        \n",
    "        out_conv = torch.cat([out_conv1, out_conv2, out_conv3], dim=1)\n",
    "        out = self.fc(out_conv.squeeze(2))\n",
    "        \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Paramters and model instance creation.\n",
    "'''\n",
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "EMBED_DIM = 50\n",
    "NUM_CLASS = len(classes)\n",
    "\n",
    "#load glove \n",
    "vectors = torchtext.vocab.GloVe(name='6B', dim=50)\n",
    "\n",
    "train_vocab = train_dataset.get_vocab()\n",
    "weights_matrix = torch.zeros((VOCAB_SIZE, EMBED_DIM))\n",
    "\n",
    "for i, word in enumerate(train_vocab.itos):\n",
    "    word_vector = torch.sum(torch.abs(vectors.get_vecs_by_tokens(word)))\n",
    "    if word_vector.item() == 0:\n",
    "        weights_matrix[i] = torch.FloatTensor(50).uniform_(-0.1, 0.1)\n",
    "    else:\n",
    "        weights_matrix[i] = vectors.get_vecs_by_tokens(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 51 minutes, 59 seconds\n",
      "\tLoss: 0.0176(train)\t|\tAcc: 90.3%(train)\n",
      "\tLoss: 0.0143(valid)\t|\tAcc: 92.1%(valid)\n",
      "Epoch: 2  | time in 42 minutes, 0 seconds\n",
      "\tLoss: 0.0109(train)\t|\tAcc: 94.0%(train)\n",
      "\tLoss: 0.0129(valid)\t|\tAcc: 92.9%(valid)\n",
      "Epoch: 3  | time in 40 minutes, 38 seconds\n",
      "\tLoss: 0.0074(train)\t|\tAcc: 96.0%(train)\n",
      "\tLoss: 0.0135(valid)\t|\tAcc: 92.9%(valid)\n",
      "Epoch: 4  | time in 39 minutes, 4 seconds\n",
      "\tLoss: 0.0048(train)\t|\tAcc: 97.4%(train)\n",
      "\tLoss: 0.0156(valid)\t|\tAcc: 92.4%(valid)\n",
      "Epoch: 5  | time in 39 minutes, 8 seconds\n",
      "\tLoss: 0.0029(train)\t|\tAcc: 98.4%(train)\n",
      "\tLoss: 0.0202(valid)\t|\tAcc: 91.7%(valid)\n",
      "Checking the results of test dataset...\n",
      "\tLoss: 0.0214(test)\t|\tAcc: 91.5%(test)"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "TRAIN_RATIO = 0.9\n",
    "\n",
    "model = TextClassifier(VOCAB_SIZE, EMBED_DIM, NUM_CLASS, weights_matrix).to(device)\n",
    "validation_loss = float('inf')\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "train_n = int(len(train_dataset) * TRAIN_RATIO)\n",
    "training_data, valid_data = random_split(train_dataset, [train_n, len(train_dataset) - train_n])\n",
    "\n",
    "for epoch in range(5):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train(training_data)\n",
    "    valid_loss, valid_acc = test(valid_data)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Project_2_(NLP_Text_Classification).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
