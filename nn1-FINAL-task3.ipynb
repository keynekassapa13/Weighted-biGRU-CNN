{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b2tWBjLyYDgH"
   },
   "source": [
    "# Project 3: Text Classification in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Let your creativity flow!\n",
    "\n",
    "As discussed earlier, you are free to come up with anything in task 3. Think and try to model unique (not too complex!) neural architecture on your own. Remember that this model has to be novel as much as possible, so try not to copy other people's existing work. Using the same data, train the new model, and report the accuracy scores. How much better/worse is this model than the previous two models? Why do you think this is better/worse?\n",
    "\n",
    "Brief description and analysis is in the report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model weighted Conv & GRU\n",
    "\n",
    "Brief description and analysis is in the report\n",
    "\n",
    "    outperform validation and test set\n",
    "    not outperform training set\n",
    "\n",
    "Epoch: 1  | time in 53 minutes, 44 seconds\n",
    "\n",
    "\tLoss: 0.0393(train)\t|\tAcc: 74.6%(train)\n",
    "\tLoss: 0.0212(valid)\t|\tAcc: 88.3%(valid)\n",
    "Epoch: 2  | time in 53 minutes, 57 seconds\n",
    "\n",
    "\tLoss: 0.0189(train)\t|\tAcc: 89.5%(train)\n",
    "\tLoss: 0.0187(valid)\t|\tAcc: 89.7%(valid)\n",
    "Epoch: 3  | time in 48 minutes, 56 seconds\n",
    "\n",
    "\tLoss: 0.0148(train)\t|\tAcc: 91.8%(train)\n",
    "\tLoss: 0.0183(valid)\t|\tAcc: 90.1%(valid)\n",
    "Epoch: 4  | time in 103 minutes, 40 seconds\n",
    "\n",
    "\tLoss: 0.0116(train)\t|\tAcc: 93.5%(train)\n",
    "\tLoss: 0.0178(valid)\t|\tAcc: 90.5%(valid)\n",
    "Epoch: 5  | time in 123 minutes, 16 seconds\n",
    "\n",
    "\tLoss: 0.0087(train)\t|\tAcc: 95.2%(train)\n",
    "\tLoss: 0.0229(valid)\t|\tAcc: 89.0%(valid)\n",
    "\n",
    "Performance on test set\n",
    "\n",
    "\tLoss: 0.0218(test)\t|\tAcc: 89.9%(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:17, 7026.61lines/s]\n",
      "120000lines [00:30, 3944.83lines/s]\n",
      "7600lines [00:01, 4705.45lines/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import text_classification\n",
    "import os\n",
    "\n",
    "NGRAMS = 2\n",
    "\n",
    "if not os.path.isdir('./.data'):\n",
    "    os.mkdir('./.data')\n",
    "\n",
    "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "    root='./.data', ngrams=NGRAMS, vocab=None)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "VOCAB_SIZE = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vocab_size = len(train_dataset.get_vocab())\n",
    "classes = train_dataset.get_labels()\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "EMBED_DIM = 32\n",
    "NUM_CLASS = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TUUvW1nOYDhk"
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    \n",
    "    label = torch.tensor([i[0] for i in batch])\n",
    "    text = [i[1] for i in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    \n",
    "    return text, offsets, label\n",
    "\n",
    "def train(train_data):\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "\n",
    "    data = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
    "    \n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = model(text, offsets)\n",
    "        \n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "        \n",
    "#         if i % 500 == 0: print(\"Batch: \", i, loss)\n",
    "    scheduler.step()\n",
    "    \n",
    "    return train_loss/len(train_data), train_acc/len(train_data)\n",
    "\n",
    "def test(test_data):\n",
    "    \n",
    "    test_loss = 0\n",
    "    acc = 0\n",
    "    \n",
    "    data = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    \n",
    "    for text, offsets, cls in data:\n",
    "        \n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "            test_loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return test_loss / len(test_data), acc / len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.bag = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, embed_dim//4, bidirectional=True)\n",
    "        \n",
    "        self.conv = nn.Conv2d(1, 8, (3, embed_dim), stride=1, bias=True)\n",
    "        self.conv1 = nn.Conv2d(1, 8, (4, embed_dim), stride=1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(1, 8, (5, embed_dim), stride=1, bias=True)\n",
    "        \n",
    "        self.fc = nn.Linear(8, 4)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.bag.weight.data.uniform_(-0.5, 0.5)\n",
    "        self.fc.weight.data.uniform_(-0.5, 0.5)\n",
    "        self.fc.bias.data.zero_()\n",
    "        self.weight_gru = torch.nn.Parameter(torch.Tensor(1, 8).uniform_(-0.5, 0.5))\n",
    "        self.weight_cnn = torch.nn.Parameter(torch.Tensor(1, 8).uniform_(-0.5, 0.5))\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        \n",
    "        text_offsets = []\n",
    "        for i in range(1, len(offsets)+1):\n",
    "            if i == len(offsets):\n",
    "                text_offsets.append(text[offsets[i-1]:])\n",
    "            else:\n",
    "                text_offsets.append(text[offsets[i-1]:offsets[i]])\n",
    "        text_len = [len(x) for x in text_offsets]\n",
    "\n",
    "        pad_text = nn.utils.rnn.pad_sequence(text_offsets)\n",
    "        \n",
    "        x = self.bag(pad_text)\n",
    "        x_packed = pack_padded_sequence(x, text_len, enforce_sorted=False)\n",
    "\n",
    "        gru_packed, hn = self.gru(x_packed)\n",
    "        \n",
    "        output, input_sizes = pad_packed_sequence(gru_packed)\n",
    "        gru_output = torch.mean(hn, dim=0).squeeze(0)\n",
    "        \n",
    "        # CONV\n",
    "        x_conv = x.transpose(0, 1)\n",
    "        x_conv = x_conv.resize(x_conv.size(0), 1, x_conv.size(1), x_conv.size(2))\n",
    "        out_conv1 = self.conv(x_conv)\n",
    "        out_conv1, _ = torch.max(out_conv1, dim=2)\n",
    "        \n",
    "        out_conv2 = self.conv(x_conv)\n",
    "        out_conv2, _ = torch.max(out_conv2, dim=2)\n",
    "        \n",
    "        out_conv3 = self.conv(x_conv)\n",
    "        out_conv3, _ = torch.max(out_conv3, dim=2)\n",
    "        \n",
    "        out_conv = torch.cat([out_conv1, out_conv2, out_conv3], dim=2)\n",
    "        out_conv = torch.mean(out_conv, dim=2)\n",
    "        \n",
    "        out_both = gru_output*self.weight_gru + out_conv*self.weight_cnn\n",
    "        \n",
    "        out = self.fc(out_both)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "EMBED_DIM = 32\n",
    "NUM_CLASS = len(classes)\n",
    "\n",
    "model = TextClassifier(VOCAB_SIZE, EMBED_DIM, NUM_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keynekassapa13/.local/lib/python3.7/site-packages/torch/tensor.py:339: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 53 minutes, 44 seconds\n",
      "\tLoss: 0.0393(train)\t|\tAcc: 74.6%(train)\n",
      "\tLoss: 0.0212(valid)\t|\tAcc: 88.3%(valid)\n",
      "Epoch: 2  | time in 53 minutes, 57 seconds\n",
      "\tLoss: 0.0189(train)\t|\tAcc: 89.5%(train)\n",
      "\tLoss: 0.0187(valid)\t|\tAcc: 89.7%(valid)\n",
      "Epoch: 3  | time in 48 minutes, 56 seconds\n",
      "\tLoss: 0.0148(train)\t|\tAcc: 91.8%(train)\n",
      "\tLoss: 0.0183(valid)\t|\tAcc: 90.1%(valid)\n",
      "Epoch: 4  | time in 103 minutes, 40 seconds\n",
      "\tLoss: 0.0116(train)\t|\tAcc: 93.5%(train)\n",
      "\tLoss: 0.0178(valid)\t|\tAcc: 90.5%(valid)\n",
      "Epoch: 5  | time in 123 minutes, 16 seconds\n",
      "\tLoss: 0.0087(train)\t|\tAcc: 95.2%(train)\n",
      "\tLoss: 0.0229(valid)\t|\tAcc: 89.0%(valid)\n",
      "Checking the results of test dataset...\n",
      "\tLoss:  0.0218(test)\t|\tAcc: 89.9%(test)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "N_EPOCHS = 5\n",
    "TRAIN_RATIO = 0.9\n",
    "\n",
    "validation_loss = float('inf')\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "train_n = int(len(train_dataset) * TRAIN_RATIO)\n",
    "training_data, valid_data = random_split(train_dataset, [train_n, len(train_dataset) - train_n])\n",
    "\n",
    "for epoch in range(5):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train(training_data)\n",
    "    valid_loss, valid_acc = test(valid_data)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')\n",
    "\n",
    "print(\"Evaluating the test data:\")\n",
    "test_loss, test_acc = test(test_dataset)\n",
    "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model weighted Conv & GRU (with GloVe)\n",
    "\n",
    "Brief description and analysis is in the report\n",
    "\n",
    "TLDR;\n",
    "    \n",
    "    outperform validation and test set\n",
    "    not outperform (but almost) training set\n",
    "\n",
    "Epoch: 1  | time in 41 minutes, 5 seconds\n",
    "\n",
    "\tLoss: 0.0167(train)\t|\tAcc: 90.9%(train)\n",
    "\tLoss: 0.0139(valid)\t|\tAcc: 92.3%(valid)\n",
    "    \n",
    "Epoch: 2  | time in 39 minutes, 24 seconds\n",
    "\n",
    "\tLoss: 0.0100(train)\t|\tAcc: 94.5%(train)\n",
    "\tLoss: 0.0128(valid)\t|\tAcc: 93.3%(valid)\n",
    "    \n",
    "Epoch: 3  | time in 39 minutes, 39 seconds\n",
    "\n",
    "\tLoss: 0.0065(train)\t|\tAcc: 96.5%(train)\n",
    "\tLoss: 0.0138(valid)\t|\tAcc: 93.1%(valid)\n",
    "    \n",
    "Epoch: 4  | time in 37 minutes, 17 seconds\n",
    "\n",
    "\tLoss: 0.0040(train)\t|\tAcc: 97.8%(train)\n",
    "\tLoss: 0.0158(valid)\t|\tAcc: 92.5%(valid)\n",
    "    \n",
    "Epoch: 5  | time in 20 minutes, 35 seconds\n",
    "\n",
    "\tLoss: 0.0023(train)\t|\tAcc: 98.8%(train)\n",
    "\tLoss: 0.0195(valid)\t|\tAcc: 92.3%(valid)\n",
    "    \n",
    "Checking the results of test dataset...\n",
    "\n",
    "\tLoss:  0.0210(test)\t|\tAcc: 91.8%(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:09, 12824.06lines/s]\n",
      "120000lines [00:17, 7008.21lines/s]\n",
      "7600lines [00:01, 4535.13lines/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the AG_NEWS dataset in bi-gram features format.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import text_classification\n",
    "import os\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch import nn\n",
    "\n",
    "NGRAMS = 1\n",
    "\n",
    "if not os.path.isdir('./.data'):\n",
    "    os.mkdir('./.data')\n",
    "\n",
    "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "    root='./.data', ngrams=NGRAMS, vocab=None)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class, pretrained_vectors):\n",
    "        super().__init__()\n",
    "        self.bag = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.bag.load_state_dict({'weight': pretrained_vectors})\n",
    "        self.gru = nn.GRU(embed_dim, embed_dim//2, bidirectional=True)\n",
    "        \n",
    "        self.conv = nn.Conv2d(1, 25, (3, embed_dim), stride=1, bias=True)\n",
    "        self.conv1 = nn.Conv2d(1, 25, (4, embed_dim), stride=1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(1, 25, (5, embed_dim), stride=1, bias=True)\n",
    "        \n",
    "        self.fc = nn.Linear(25, 4)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.fc.weight.data.uniform_(-0.5, 0.5)\n",
    "        self.fc.bias.data.zero_()\n",
    "        self.weight_gru = torch.nn.Parameter(torch.Tensor(1, 25).uniform_(-0.5, 0.5))\n",
    "        self.weight_cnn = torch.nn.Parameter(torch.Tensor(1, 25).uniform_(-0.5, 0.5))\n",
    "    \n",
    "    def forward(self, text, offsets):\n",
    "        \n",
    "        text_offsets = []\n",
    "        for i in range(1, len(offsets)+1):\n",
    "            if i == len(offsets):\n",
    "                text_offsets.append(text[offsets[i-1]:])\n",
    "            else:\n",
    "                text_offsets.append(text[offsets[i-1]:offsets[i]])\n",
    "        text_len = [len(x) for x in text_offsets]\n",
    "\n",
    "        pad_text = nn.utils.rnn.pad_sequence(text_offsets)\n",
    "        \n",
    "        x = self.bag(pad_text)\n",
    "        x_packed = pack_padded_sequence(x, text_len, enforce_sorted=False)\n",
    "\n",
    "        gru_packed, hn = self.gru(x_packed)\n",
    "        \n",
    "        output, input_sizes = pad_packed_sequence(gru_packed)\n",
    "        gru_output = torch.mean(hn, dim=0).squeeze(0)\n",
    "        \n",
    "        # CONV\n",
    "        x_conv = x.transpose(0, 1)\n",
    "        x_conv = x_conv.resize(x_conv.size(0), 1, x_conv.size(1), x_conv.size(2))\n",
    "        out_conv1 = self.conv(x_conv)\n",
    "        out_conv1, _ = torch.max(out_conv1, dim=2)\n",
    "        \n",
    "        out_conv2 = self.conv(x_conv)\n",
    "        out_conv2, _ = torch.max(out_conv2, dim=2)\n",
    "        \n",
    "        out_conv3 = self.conv(x_conv)\n",
    "        out_conv3, _ = torch.max(out_conv3, dim=2)\n",
    "        \n",
    "        out_conv = torch.cat([out_conv1, out_conv2, out_conv3], dim=2)\n",
    "        out_conv = torch.mean(out_conv, dim=2)\n",
    "        \n",
    "        out_both = gru_output*self.weight_gru + out_conv*self.weight_cnn\n",
    "        out = self.fc(out_both)\n",
    "\n",
    "        return out\n",
    "\n",
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "EMBED_DIM = 50\n",
    "NUM_CLASS = len(classes)\n",
    "\n",
    "#load glove \n",
    "vectors = torchtext.vocab.GloVe(name='6B', dim=50)\n",
    "\n",
    "train_vocab = train_dataset.get_vocab()\n",
    "weights_matrix = torch.zeros((VOCAB_SIZE, EMBED_DIM))\n",
    "\n",
    "for i, word in enumerate(train_vocab.itos):\n",
    "    word_vector = torch.sum(torch.abs(vectors.get_vecs_by_tokens(word)))\n",
    "    if word_vector.item() == 0:\n",
    "        weights_matrix[i] = torch.FloatTensor(50).uniform_(-0.1, 0.1)\n",
    "    else:\n",
    "        weights_matrix[i] = vectors.get_vecs_by_tokens(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 41 minutes, 5 seconds\n",
      "\tLoss: 0.0167(train)\t|\tAcc: 90.9%(train)\n",
      "\tLoss: 0.0139(valid)\t|\tAcc: 92.3%(valid)\n",
      "Epoch: 2  | time in 39 minutes, 24 seconds\n",
      "\tLoss: 0.0100(train)\t|\tAcc: 94.5%(train)\n",
      "\tLoss: 0.0128(valid)\t|\tAcc: 93.3%(valid)\n",
      "Epoch: 3  | time in 39 minutes, 39 seconds\n",
      "\tLoss: 0.0065(train)\t|\tAcc: 96.5%(train)\n",
      "\tLoss: 0.0138(valid)\t|\tAcc: 93.1%(valid)\n",
      "Epoch: 4  | time in 37 minutes, 17 seconds\n",
      "\tLoss: 0.0040(train)\t|\tAcc: 97.8%(train)\n",
      "\tLoss: 0.0158(valid)\t|\tAcc: 92.5%(valid)\n",
      "Epoch: 5  | time in 20 minutes, 35 seconds\n",
      "\tLoss: 0.0023(train)\t|\tAcc: 98.8%(train)\n",
      "\tLoss: 0.0195(valid)\t|\tAcc: 92.3%(valid)\n",
      "Checking the results of test dataset...\n",
      "\tLoss:  0.0210(test)\t|\tAcc: 91.8%(test)"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "TRAIN_RATIO = 0.9\n",
    "\n",
    "model = TextClassifier(VOCAB_SIZE, EMBED_DIM, NUM_CLASS, weights_matrix).to(device)\n",
    "validation_loss = float('inf')\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "train_n = int(len(train_dataset) * TRAIN_RATIO)\n",
    "training_data, valid_data = random_split(train_dataset, [train_n, len(train_dataset) - train_n])\n",
    "\n",
    "for epoch in range(5):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train(training_data)\n",
    "    valid_loss, valid_acc = test(valid_data)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')\n",
    "    \n",
    "print(\"Evaluating the test data:\")\n",
    "test_loss, test_acc = test(test_dataset)\n",
    "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Project_2_(NLP_Text_Classification).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
